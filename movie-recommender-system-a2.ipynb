{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommender System - Collaborative Filtering with LightFM\n",
    "\n",
    "## AI Task: Personalized Top-K Recommendation\n",
    "\n",
    "Build a **collaborative filtering** recommender using **matrix factorization** to predict personalized movie recommendations. The system learns latent user preferences and movie characteristics from implicit feedback signals (watch duration, completion rate) to rank unwatched movies and generate top-k recommendations.\n",
    "\n",
    "**Task Type**: Implicit collaborative filtering with top-k ranking optimization\n",
    "\n",
    "## Problem Context\n",
    "\n",
    "Users don't provide explicit ratings - we only observe behavioral signals:\n",
    "- Watch duration (minutes watched per session)\n",
    "- Completion percentage (how much of the movie was watched)\n",
    "- Multiple viewing sessions (rewatches indicate strong interest)\n",
    "\n",
    "**Challenge**: Convert sparse behavioral data into reliable preference signals for recommendation.\n",
    "\n",
    "## Approach Overview\n",
    "\n",
    "### 1. Data Understanding\n",
    "- **Input**: User watch history (105K events), movie metadata (1K movies), user profiles (10K users)\n",
    "- **Challenge**: Sparse interaction matrix (~1% density), no explicit ratings\n",
    "- **Preprocessing**: Aggregate sessions, filter cold-start users/items, normalize ranges\n",
    "\n",
    "### 2. Implicit Feedback Signal Engineering\n",
    "Construct a **strength score** from behavioral data:\n",
    "```\n",
    "strength = log(1 + total_minutes_watched) * (0.3 + 0.7 * completion_rate)\n",
    "```\n",
    "**Rationale**:\n",
    "- **Log transform**: Diminishing returns for very long watch times (prevents outliers)\n",
    "- **Completion rate**: 70% weight (finishing indicates genuine interest vs. abandoning)\n",
    "- **Watch duration**: 30% weight (accounts for rewatches and movie length)\n",
    "\n",
    "### 3. Algorithm: LightFM with BPR Loss\n",
    "\n",
    "**Method**: Hybrid matrix factorization with gradient descent optimization\n",
    "\n",
    "**BPR Loss (Bayesian Personalized Ranking)**:\n",
    "- Pairwise ranking optimization: learns to rank watched items higher than unwatched\n",
    "- Samples positive (watched) and negative (unwatched) pairs per user\n",
    "- Robust to sparse implicit data with stable convergence\n",
    "\n",
    "**Model Architecture**:\n",
    "- User embeddings: [num_users × num_components] learned latent factors\n",
    "- Item embeddings: [num_items × num_components] learned latent factors  \n",
    "- Prediction: dot product of user/item embeddings + biases\n",
    "- Training: Stochastic gradient descent with negative sampling\n",
    "\n",
    "### 4. Evaluation Strategy\n",
    "- **Split**: 80/20 random holdout (train on 80%, test on 20%)\n",
    "- **Metrics**: \n",
    "  - **Precision@10**: Fraction of top-10 that are relevant (accuracy-based)\n",
    "  - **Recall@10**: Fraction of user's test items recovered in top-10 (coverage)\n",
    "  - **nDCG@10**: Ranking quality with position weighting (ranking-based)\n",
    "  - **AUC**: Overall ranking quality (1.0 = perfect, 0.5 = random)\n",
    "\n",
    "### 5. Expected Performance\n",
    "Targets for implicit feedback with ~1% matrix density:\n",
    "- **Test Precision@10**: 0.2-2% (baseline: random ~0.1%)\n",
    "- **Test Recall@10**: 1-5%\n",
    "- **Test nDCG@10**: 0.01-0.05\n",
    "- **Test AUC**: 0.50-0.60 (baseline: random = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:34:29.167428Z",
     "iopub.status.busy": "2025-10-26T02:34:29.167097Z",
     "iopub.status.idle": "2025-10-26T02:34:33.311945Z",
     "shell.execute_reply": "2025-10-26T02:34:33.310240Z",
     "shell.execute_reply.started": "2025-10-26T02:34:29.167404Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install lightfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Core dependencies for the collaborative filtering recommender system:\n",
    "- **pandas/numpy**: Data manipulation and numerical operations\n",
    "- **scipy.sparse**: Memory-efficient sparse matrix storage (CSR format for ~1% density data)\n",
    "- **LightFM**: Hybrid recommender with BPR loss for implicit feedback\n",
    "- **matplotlib**: Visualization utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-26T02:34:33.314417Z",
     "iopub.status.busy": "2025-10-26T02:34:33.314085Z",
     "iopub.status.idle": "2025-10-26T02:34:33.322329Z",
     "shell.execute_reply": "2025-10-26T02:34:33.321229Z",
     "shell.execute_reply.started": "2025-10-26T02:34:33.314382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Sparse matrix operations\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# LightFM recommender system\n",
    "from lightfm import LightFM\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from lightfm.evaluation import precision_at_k, recall_at_k, auc_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress pandas RuntimeWarning for NaN display issues\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "### Data Preprocessing Parameters\n",
    "- **MIN_USER_INTERACTIONS**: Filter users with fewer than 3 movies watched (cold-start prevention)\n",
    "- **MIN_ITEM_INTERACTIONS**: Filter movies with fewer than 5 watchers (rare item removal)\n",
    "- **COMPLETION_WEIGHT**: Emphasis on completion vs. watch time (0.7 = 70% weight on finishing movies)\n",
    "\n",
    "### Model Hyperparameters\n",
    "- **NO_COMPONENTS**: Dimensionality of user/movie embeddings (latent factors)\n",
    "- **LEARNING_RATE**: SGD step size for gradient descent optimization\n",
    "- **EPOCHS**: Number of training iterations through the data\n",
    "- **LOSS**: Loss function ('bpr' for Bayesian Personalized Ranking)\n",
    "\n",
    "### Evaluation Parameters\n",
    "- **K**: Top-k for Precision@k, Recall@k, and nDCG@k metrics\n",
    "- **TEST_PERCENTAGE**: Fraction of interactions held out for testing (0.2 = 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:34:33.323874Z",
     "iopub.status.busy": "2025-10-26T02:34:33.323589Z",
     "iopub.status.idle": "2025-10-26T02:34:33.344272Z",
     "shell.execute_reply": "2025-10-26T02:34:33.343362Z",
     "shell.execute_reply.started": "2025-10-26T02:34:33.323852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Data paths (local raw_data folder)\n",
    "DATA_DIR = Path('./raw_data')\n",
    "\n",
    "# Data filtering thresholds\n",
    "MIN_USER_INTERACTIONS = 3  # Remove users with fewer than 3 movies watched\n",
    "MIN_ITEM_INTERACTIONS = 5  # Remove movies with fewer than 5 watchers\n",
    "COMPLETION_WEIGHT = 0.7    # Weight for completion rate in strength score (0-1)\n",
    "\n",
    "# LightFM model hyperparameters\n",
    "NO_COMPONENTS = 50     # Latent factor dimensions (appropriate for sparse data)\n",
    "LEARNING_RATE = 0.01   # Conservative learning rate for stable convergence\n",
    "EPOCHS = 10            # Training iterations\n",
    "LOSS = 'bpr'           # BPR loss function (Bayesian Personalized Ranking)\n",
    "\n",
    "# Evaluation parameters\n",
    "K = 10                 # Top-k for Precision@k, Recall@k, nDCG@k\n",
    "TEST_PERCENTAGE = 0.2  # Fraction of data held out for testing\n",
    "\n",
    "print(\"Configuration set!\")\n",
    "print(f\"  Model: {NO_COMPONENTS} factors, {EPOCHS} epochs, {LOSS.upper()} loss, LR={LEARNING_RATE}\")\n",
    "print(f\"  Filter: >={MIN_USER_INTERACTIONS} user interactions, >={MIN_ITEM_INTERACTIONS} item interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Validate Data\n",
    "\n",
    "Load raw CSV files and perform data quality checks:\n",
    "1. Remove rows with NaN or infinite values\n",
    "2. Validate numeric ranges (watch duration ≥ 0, progress 0-100%)\n",
    "3. Convert progress_percentage (0-100 scale) to decimal (0-1 scale) for calculations\n",
    "\n",
    "**Expected columns**:\n",
    "- `users.csv`: user_id, subscription_type, demographics\n",
    "- `movies.csv`: movie_id, title, genre_primary, genre_secondary, duration_minutes\n",
    "- `watch_history.csv`: user_id, movie_id, watch_duration_minutes, progress_percentage, session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:34:33.346727Z",
     "iopub.status.busy": "2025-10-26T02:34:33.346371Z",
     "iopub.status.idle": "2025-10-26T02:34:33.909985Z",
     "shell.execute_reply": "2025-10-26T02:34:33.908851Z",
     "shell.execute_reply.started": "2025-10-26T02:34:33.346672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADING & VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print('Loading data...')\n",
    "# Load raw CSV files\n",
    "users = pd.read_csv(DATA_DIR / 'users.csv')\n",
    "movies = pd.read_csv(DATA_DIR / 'movies.csv')\n",
    "watch_history = pd.read_csv(DATA_DIR / 'watch_history.csv')\n",
    "\n",
    "print(f'Loaded {len(users):,} users, {len(movies):,} movies, {len(watch_history):,} watch events')\n",
    "\n",
    "# Data validation and cleaning\n",
    "print('\\nValidating data quality...')\n",
    "initial_count = len(watch_history)\n",
    "\n",
    "# Replace infinite values with NaN for consistent handling\n",
    "watch_history = watch_history.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Identify which columns exist in the dataset\n",
    "required_cols = ['user_id', 'movie_id']  # Must have\n",
    "optional_cols = ['watch_duration_minutes', 'progress_percentage', 'session_id']\n",
    "existing_cols = [col for col in required_cols + optional_cols if col in watch_history.columns]\n",
    "\n",
    "# Drop rows with NaN in critical columns\n",
    "watch_history = watch_history.dropna(subset=existing_cols)\n",
    "\n",
    "# Validate numeric ranges\n",
    "if 'watch_duration_minutes' in watch_history.columns:\n",
    "    # Watch duration must be non-negative\n",
    "    watch_history = watch_history[watch_history['watch_duration_minutes'] >= 0]\n",
    "\n",
    "if 'progress_percentage' in watch_history.columns:\n",
    "    # Convert progress from percentage (0-100) to decimal (0-1)\n",
    "    watch_history['progress_decimal'] = watch_history['progress_percentage'] / 100\n",
    "    \n",
    "    # Filter valid range [0, 100]%\n",
    "    watch_history = watch_history[\n",
    "        (watch_history['progress_percentage'] >= 0) & \n",
    "        (watch_history['progress_percentage'] <= 100)\n",
    "    ]\n",
    "\n",
    "# Report cleaning results\n",
    "removed_count = initial_count - len(watch_history)\n",
    "if removed_count > 0:\n",
    "    print(f'Removed {removed_count:,} invalid rows ({removed_count/initial_count*100:.2f}%)')\n",
    "else:\n",
    "    print('All rows valid!')\n",
    "\n",
    "print(f'Final dataset: {len(watch_history):,} valid watch events')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Watch Sessions\n",
    "\n",
    "**Problem**: Users may watch the same movie multiple times across different sessions.\n",
    "\n",
    "**Solution**: Aggregate all sessions per user-movie pair into a single **strength score**.\n",
    "\n",
    "**Strength Formula**:\n",
    "```\n",
    "strength = log(1 + total_minutes) * (0.3 + 0.7 * completion_rate)\n",
    "```\n",
    "\n",
    "**Rationale**:\n",
    "- **log(1 + total_minutes)**: Diminishing returns for very long watch times (prevents outliers from dominating)\n",
    "- **completion_rate**: Mean of progress_percentage across all sessions (0-1 scale)\n",
    "- **0.7 weight on completion**: Watching to the end signals strong interest vs. abandoning early\n",
    "- **0.3 weight on duration**: Still accounts for rewatching behavior and partial views\n",
    "\n",
    "**Example**: User watches \"Movie A\" twice - 30 min (50% complete) and 60 min (100% complete)\n",
    "- total_minutes = 90, completion_mean = 0.75\n",
    "- strength = log(91) * (0.3 + 0.7 * 0.75) = 4.51 * 0.825 = 3.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:34:33.911503Z",
     "iopub.status.busy": "2025-10-26T02:34:33.911156Z",
     "iopub.status.idle": "2025-10-26T02:34:33.982023Z",
     "shell.execute_reply": "2025-10-26T02:34:33.980985Z",
     "shell.execute_reply.started": "2025-10-26T02:34:33.911470Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGGREGATE WATCH SESSIONS\n",
    "# ============================================================================\n",
    "\n",
    "print('\\nAggregating watch sessions per user-movie pair...')\n",
    "\n",
    "# Group by (user_id, movie_id) and aggregate metrics\n",
    "aggregated = watch_history.groupby(['user_id', 'movie_id']).agg({\n",
    "    'watch_duration_minutes': 'sum',      # Total time spent watching\n",
    "    'progress_decimal': 'mean',           # Average completion rate (0-1)\n",
    "    'session_id': 'count'                 # Number of viewing sessions\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "aggregated.columns = ['user_id', 'movie_id', 'total_minutes', 'completion_mean', 'num_sessions']\n",
    "\n",
    "# Calculate implicit feedback strength score\n",
    "# Formula: log(1 + minutes) * (base_weight + completion_weight * completion_rate)\n",
    "# - log transform: diminishing returns for very long watch times\n",
    "# - completion_mean: 0-1 scale (0% to 100% watched)\n",
    "# - COMPLETION_WEIGHT: emphasis on finishing vs. just starting\n",
    "aggregated['strength'] = (\n",
    "    np.log1p(aggregated['total_minutes']) *           # log(1 + x) for numerical stability\n",
    "    (0.3 + COMPLETION_WEIGHT * aggregated['completion_mean'])  # 30% base + 70% completion\n",
    ")\n",
    "\n",
    "print(f'Aggregated to {len(aggregated):,} unique user-movie interactions')\n",
    "print(f'  Strength range: [{aggregated[\"strength\"].min():.2f}, {aggregated[\"strength\"].max():.2f}]')\n",
    "print(f'  Avg sessions per interaction: {aggregated[\"num_sessions\"].mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Sparse Data\n",
    "\n",
    "**Cold Start Problem**: Users with very few interactions and rarely-watched movies hurt model quality.\n",
    "\n",
    "**Solution**: \n",
    "- Remove users with fewer than 3 movies watched (insufficient data to learn preferences)\n",
    "- Remove movies with fewer than 5 watchers (too rare to recommend reliably)\n",
    "\n",
    "This filtering improves matrix density and model generalization at the cost of coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:34:33.983311Z",
     "iopub.status.busy": "2025-10-26T02:34:33.983033Z",
     "iopub.status.idle": "2025-10-26T02:34:34.050884Z",
     "shell.execute_reply": "2025-10-26T02:34:34.049888Z",
     "shell.execute_reply.started": "2025-10-26T02:34:33.983281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FILTER SPARSE INTERACTIONS\n",
    "# ============================================================================\n",
    "\n",
    "print('\\nFiltering sparse interactions...')\n",
    "\n",
    "# Count how many interactions each user/movie has\n",
    "user_counts = aggregated.groupby('user_id').size()   # Movies per user\n",
    "item_counts = aggregated.groupby('movie_id').size()  # Users per movie\n",
    "\n",
    "# Keep only users/movies that meet minimum thresholds\n",
    "valid_users = user_counts[user_counts >= MIN_USER_INTERACTIONS].index\n",
    "valid_items = item_counts[item_counts >= MIN_ITEM_INTERACTIONS].index\n",
    "\n",
    "# Filter the interaction data\n",
    "filtered = aggregated[\n",
    "    aggregated['user_id'].isin(valid_users) &\n",
    "    aggregated['movie_id'].isin(valid_items)\n",
    "].copy()  # .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Report filtering results\n",
    "print(f'Filtered: {len(aggregated):,} -> {len(filtered):,} interactions')\n",
    "print(f'  Users: {len(aggregated[\"user_id\"].unique()):,} -> {len(filtered[\"user_id\"].unique()):,}')\n",
    "print(f'  Movies: {len(aggregated[\"movie_id\"].unique()):,} -> {len(filtered[\"movie_id\"].unique()):,}')\n",
    "\n",
    "# Calculate sparsity metrics\n",
    "total_possible = len(filtered[\"user_id\"].unique()) * len(filtered[\"movie_id\"].unique())\n",
    "density_pct = (len(filtered) / total_possible) * 100\n",
    "print(f'  Matrix density: {density_pct:.4f}% (sparsity: {100-density_pct:.4f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Interaction Matrix\n",
    "\n",
    "Convert user-movie interactions into a **sparse CSR (Compressed Sparse Row) matrix**.\n",
    "\n",
    "**Why sparse?** With ~1% density (99% of cells are zero), storing a dense matrix would waste 99% of memory.\n",
    "\n",
    "**Matrix structure**:\n",
    "- Rows = users (index 0 to num_users-1)\n",
    "- Columns = movies (index 0 to num_items-1)\n",
    "- Values = strength scores (implicit feedback signal)\n",
    "\n",
    "**Mappings**: Create bidirectional dictionaries to convert between:\n",
    "- Original IDs (e.g., 'user_00123') ↔ Matrix indices (0, 1, 2, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:34:34.052308Z",
     "iopub.status.busy": "2025-10-26T02:34:34.051979Z",
     "iopub.status.idle": "2025-10-26T02:34:34.100043Z",
     "shell.execute_reply": "2025-10-26T02:34:34.099119Z",
     "shell.execute_reply.started": "2025-10-26T02:34:34.052283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BUILD SPARSE INTERACTION MATRIX\n",
    "# ============================================================================\n",
    "\n",
    "print('\\nBuilding sparse interaction matrix...')\n",
    "\n",
    "# Get unique user and movie IDs, sorted for reproducibility\n",
    "unique_users = sorted(filtered['user_id'].unique())\n",
    "unique_items = sorted(filtered['movie_id'].unique())\n",
    "\n",
    "# Create bidirectional mappings: ID <-> index\n",
    "user_to_idx = {user: idx for idx, user in enumerate(unique_users)}\n",
    "item_to_idx = {item: idx for idx, item in enumerate(unique_items)}\n",
    "idx_to_user = {idx: user for user, idx in user_to_idx.items()}  # Reverse lookup\n",
    "idx_to_item = {idx: item for item, idx in item_to_idx.items()}  # Reverse lookup\n",
    "\n",
    "# Map string IDs to integer indices for matrix construction\n",
    "filtered['user_idx'] = filtered['user_id'].map(user_to_idx)\n",
    "filtered['item_idx'] = filtered['movie_id'].map(item_to_idx)\n",
    "\n",
    "# Matrix dimensions\n",
    "num_users = len(unique_users)\n",
    "num_items = len(unique_items)\n",
    "\n",
    "# Build sparse CSR matrix\n",
    "# CSR (Compressed Sparse Row) is efficient for:\n",
    "# - Row slicing (get all items for a user)\n",
    "# - Matrix-vector multiplication (used in recommendations)\n",
    "interaction_matrix = csr_matrix(\n",
    "    (filtered['strength'].values,                           # Data: strength scores\n",
    "     (filtered['user_idx'].values, filtered['item_idx'].values)),  # (row, col) indices\n",
    "    shape=(num_users, num_items)                            # Matrix dimensions\n",
    ")\n",
    "\n",
    "# Calculate density metrics\n",
    "density = (interaction_matrix.nnz / (num_users * num_items)) * 100\n",
    "\n",
    "print(f'Matrix: {num_users:,} users x {num_items:,} items')\n",
    "print(f'  Non-zero entries: {interaction_matrix.nnz:,}')\n",
    "print(f'  Density: {density:.4f}% (sparse)')\n",
    "print(f'  Memory saved vs. dense: {(1 - density/100) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "Split interactions into training (80%) and testing (20%) sets for evaluation.\n",
    "\n",
    "**Method**: Random holdout - each interaction has 20% probability of being assigned to test set.\n",
    "\n",
    "**Why random vs. temporal?** \n",
    "- **Random**: Evaluates general prediction ability across all users/items\n",
    "- **Temporal** (not used): Would test predicting future interactions from past behavior\n",
    "\n",
    "The random split provides unbiased evaluation of the model's collaborative filtering capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:34:34.116028Z",
     "iopub.status.busy": "2025-10-26T02:34:34.115542Z",
     "iopub.status.idle": "2025-10-26T02:34:34.133403Z",
     "shell.execute_reply": "2025-10-26T02:34:34.132076Z",
     "shell.execute_reply.started": "2025-10-26T02:34:34.116003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN/TEST SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "print('\\nSplitting train/test sets...')\n",
    "\n",
    "# Randomly split interactions: 80% train, 20% test\n",
    "# random_state=42 for reproducibility\n",
    "train, test = random_train_test_split(\n",
    "    interaction_matrix,\n",
    "    test_percentage=TEST_PERCENTAGE,  # 0.2 = 20%\n",
    "    random_state=42                   # Fixed seed for reproducible results\n",
    ")\n",
    "\n",
    "print(f'Train: {train.nnz:,} interactions ({(train.nnz/interaction_matrix.nnz)*100:.1f}%)')\n",
    "print(f'Test:  {test.nnz:,} interactions ({(test.nnz/interaction_matrix.nnz)*100:.1f}%)')\n",
    "print(f'  Note: Model trained on train set, evaluated on held-out test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LightFM Model\n",
    "\n",
    "**BPR Loss (Bayesian Personalized Ranking)**:\n",
    "- Pairwise ranking optimization: learns to rank watched items higher than unwatched items\n",
    "- Samples positive (watched) and negative (unwatched) pairs per user during training\n",
    "- Optimizes ranking quality while preventing overfitting on sparse implicit data\n",
    "\n",
    "**Training Process**:\n",
    "- SGD (Stochastic Gradient Descent) with specified learning rate\n",
    "- Each epoch = one complete pass through all training interactions\n",
    "- Progress tracked every 5 epochs to monitor convergence and detect overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:34:34.135845Z",
     "iopub.status.busy": "2025-10-26T02:34:34.135524Z",
     "iopub.status.idle": "2025-10-26T02:35:13.099531Z",
     "shell.execute_reply": "2025-10-26T02:35:13.098530Z",
     "shell.execute_reply.started": "2025-10-26T02:34:34.135808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN LIGHTFM MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(f'\\nTraining LightFM with {LOSS.upper()} loss...')\n",
    "print(f'Hyperparameters: {NO_COMPONENTS} factors, {EPOCHS} epochs, lr={LEARNING_RATE}')\n",
    "\n",
    "# Initialize LightFM model\n",
    "model = LightFM(\n",
    "    loss=LOSS,                    # Loss function ('bpr' or 'warp')\n",
    "    no_components=NO_COMPONENTS,  # Embedding dimensionality (latent factors)\n",
    "    learning_rate=LEARNING_RATE,  # SGD step size\n",
    "    random_state=42               # Reproducible initialization\n",
    ")\n",
    "\n",
    "# Train with progress monitoring\n",
    "print('\\nTraining progress:')\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train for 1 epoch (fit_partial allows incremental training)\n",
    "    model.fit_partial(train, epochs=1)\n",
    "    \n",
    "    # Evaluate every 5 epochs to monitor convergence\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        train_p = precision_at_k(model, train, k=K).mean()  # Train performance\n",
    "        test_p = precision_at_k(model, test, k=K).mean()    # Test performance\n",
    "        \n",
    "        gap = train_p - test_p\n",
    "        print(f'  Epoch {epoch+1:2d}: Train P@{K}={train_p:.4f}, Test P@{K}={test_p:.4f} (gap: {gap:.4f})')\n",
    "\n",
    "print('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "**Metrics Explained**:\n",
    "\n",
    "1. **Precision@10**: Of the 10 recommended items, what fraction are relevant (appear in test set)?\n",
    "   - Higher = fewer irrelevant recommendations\n",
    "   - Measures accuracy of top-k predictions\n",
    "\n",
    "2. **Recall@10**: Of all relevant items (in test set), what fraction appear in top-10 recommendations?\n",
    "   - Higher = better coverage of user's interests\n",
    "   - Measures completeness of top-k predictions\n",
    "\n",
    "3. **nDCG@10 (Normalized Discounted Cumulative Gain)**: Ranking quality with position weighting\n",
    "   - Rewards items ranked higher (position 1 > position 10)\n",
    "   - Range: 0.0 (worst) to 1.0 (perfect ranking)\n",
    "   - Industry standard metric for top-k recommendation evaluation\n",
    "\n",
    "4. **AUC (Area Under ROC Curve)**: Overall ranking quality across all items\n",
    "   - 0.5 = random guessing (no signal)\n",
    "   - 1.0 = perfect ranking (all relevant items ranked higher than irrelevant)\n",
    "   - Measures if model consistently ranks relevant items above irrelevant ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:35:13.101895Z",
     "iopub.status.busy": "2025-10-26T02:35:13.101083Z",
     "iopub.status.idle": "2025-10-26T02:35:26.252634Z",
     "shell.execute_reply": "2025-10-26T02:35:26.251538Z",
     "shell.execute_reply.started": "2025-10-26T02:35:13.101863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVALUATE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('EVALUATION RESULTS')\n",
    "print('='*70)\n",
    "\n",
    "# Calculate all metrics on train and test sets\n",
    "train_precision = precision_at_k(model, train, k=K).mean()\n",
    "test_precision = precision_at_k(model, test, k=K).mean()\n",
    "\n",
    "train_recall = recall_at_k(model, train, k=K).mean()\n",
    "test_recall = recall_at_k(model, test, k=K).mean()\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test).mean()\n",
    "\n",
    "# Calculate nDCG@k (Normalized Discounted Cumulative Gain)\n",
    "def ndcg_at_k(model, interactions, k=10):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain at k.\n",
    "    Rewards items ranked higher in the top-k list.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LightFM model\n",
    "        interactions: Sparse interaction matrix (train or test)\n",
    "        k: Number of recommendations to consider\n",
    "    \n",
    "    Returns:\n",
    "        Mean nDCG@k across all users\n",
    "    \"\"\"\n",
    "    # Convert to CSR format for efficient row indexing\n",
    "    interactions = interactions.tocsr()\n",
    "    n_users, n_items = interactions.shape\n",
    "    ndcg_scores = []\n",
    "    \n",
    "    for user_id in range(n_users):\n",
    "        # Get actual relevant items for this user\n",
    "        actual_items = interactions[user_id].indices\n",
    "        \n",
    "        if len(actual_items) == 0:\n",
    "            continue  # Skip users with no interactions\n",
    "        \n",
    "        # Get predictions for all items\n",
    "        item_indices = np.arange(n_items)\n",
    "        user_indices = np.full(n_items, user_id)\n",
    "        scores = model.predict(user_indices, item_indices)\n",
    "        \n",
    "        # Get top-k predicted items\n",
    "        top_k_items = np.argsort(-scores)[:k]\n",
    "        \n",
    "        # Calculate DCG@k (Discounted Cumulative Gain)\n",
    "        dcg = 0.0\n",
    "        for i, item in enumerate(top_k_items):\n",
    "            if item in actual_items:\n",
    "                # Relevance = 1 if item is relevant, 0 otherwise\n",
    "                # Position discount: 1/log2(position + 1)\n",
    "                dcg += 1.0 / np.log2(i + 2)  # i+2 because position is 1-indexed\n",
    "        \n",
    "        # Calculate IDCG@k (Ideal DCG - best possible ranking)\n",
    "        ideal_k = min(len(actual_items), k)\n",
    "        idcg = sum(1.0 / np.log2(i + 2) for i in range(ideal_k))\n",
    "        \n",
    "        # nDCG = DCG / IDCG (normalize to 0-1 range)\n",
    "        if idcg > 0:\n",
    "            ndcg_scores.append(dcg / idcg)\n",
    "    \n",
    "    return np.mean(ndcg_scores) if ndcg_scores else 0.0\n",
    "\n",
    "train_ndcg = ndcg_at_k(model, train, k=K)\n",
    "test_ndcg = ndcg_at_k(model, test, k=K)\n",
    "\n",
    "# Display results\n",
    "print(f'\\nPrecision@{K}: (fraction of top-{K} that are relevant)')\n",
    "print(f'  Train: {train_precision:.4f} ({train_precision*100:.2f}%)')\n",
    "print(f'  Test:  {test_precision:.4f} ({test_precision*100:.2f}%)')\n",
    "\n",
    "print(f'\\nRecall@{K}: (fraction of relevant items in top-{K})')\n",
    "print(f'  Train: {train_recall:.4f} ({train_recall*100:.2f}%)')\n",
    "print(f'  Test:  {test_recall:.4f} ({test_recall*100:.2f}%)')\n",
    "\n",
    "print(f'\\nnDCG@{K}: (ranking quality with position weighting, 1.0=perfect)')\n",
    "print(f'  Train: {train_ndcg:.4f}')\n",
    "print(f'  Test:  {test_ndcg:.4f}')\n",
    "\n",
    "print(f'\\nAUC Score: (ranking quality, 0.5=random, 1.0=perfect)')\n",
    "print(f'  Train: {train_auc:.4f}')\n",
    "print(f'  Test:  {test_auc:.4f}')\n",
    "\n",
    "# Calculate train/test gaps\n",
    "precision_gap = train_precision - test_precision\n",
    "recall_gap = train_recall - test_recall\n",
    "ndcg_gap = train_ndcg - test_ndcg\n",
    "auc_gap = train_auc - test_auc\n",
    "\n",
    "print(f'\\nTrain/Test Gaps:')\n",
    "print(f'  Precision@{K} gap: {precision_gap:.4f}')\n",
    "print(f'  Recall@{K} gap: {recall_gap:.4f}')\n",
    "print(f'  nDCG@{K} gap: {ndcg_gap:.4f}')\n",
    "print(f'  AUC gap: {auc_gap:.4f}')\n",
    "\n",
    "print('\\n' + '='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Recommendations\n",
    "\n",
    "Generate top-10 movie recommendations for 3 randomly selected users.\n",
    "\n",
    "**Recommendation Process**:\n",
    "1. Predict scores for all movies using the trained model\n",
    "2. Exclude movies the user has already watched (collaborative filtering suggests *new* content)\n",
    "3. Rank remaining movies by predicted score (descending)\n",
    "4. Return top-k items with metadata (title, genres, scores)\n",
    "\n",
    "**Note**: These are sample outputs for demonstration purposes. Actual recommendation quality is measured by the test metrics above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T02:38:12.075586Z",
     "iopub.status.busy": "2025-10-26T02:38:12.074674Z",
     "iopub.status.idle": "2025-10-26T02:38:12.152078Z",
     "shell.execute_reply": "2025-10-26T02:38:12.151284Z",
     "shell.execute_reply.started": "2025-10-26T02:38:12.075556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GENERATE SAMPLE RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print('\\nGenerating sample recommendations...')\n",
    "\n",
    "def get_recommendations(user_idx, model, interaction_matrix, idx_to_item, movies_df, k=10):\n",
    "    '''\n",
    "    Generate top-k movie recommendations for a user.\n",
    "    \n",
    "    Args:\n",
    "        user_idx: Integer index of user (not original user_id)\n",
    "        model: Trained LightFM model\n",
    "        interaction_matrix: Sparse matrix of user-item interactions\n",
    "        idx_to_item: Dictionary mapping matrix indices to movie IDs\n",
    "        movies_df: DataFrame with movie metadata\n",
    "        k: Number of recommendations to return\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with movie_id, title, genres, score\n",
    "    '''\n",
    "    n_items = interaction_matrix.shape[1]\n",
    "    \n",
    "    # Predict scores for all items\n",
    "    # LightFM.predict() requires parallel arrays of user indices and item indices\n",
    "    item_indices = np.arange(n_items)              # [0, 1, 2, ..., n_items-1]\n",
    "    user_indices = np.full(n_items, user_idx)      # [user_idx, user_idx, ..., user_idx]\n",
    "    scores = model.predict(user_indices, item_indices)\n",
    "    \n",
    "    # Exclude items user has already watched\n",
    "    known_items = interaction_matrix[user_idx].indices  # Get non-zero columns for this user\n",
    "    scores[known_items] = -np.inf                       # Mask with -infinity to exclude\n",
    "    \n",
    "    # Get top-k items by score\n",
    "    top_indices = np.argsort(-scores)[:k]  # Sort descending, take first k\n",
    "    top_scores = scores[top_indices]\n",
    "    \n",
    "    # Map back to movie IDs and fetch metadata\n",
    "    recommendations = []\n",
    "    for idx, score in zip(top_indices, top_scores):\n",
    "        movie_id = idx_to_item[idx]\n",
    "        movie_info = movies_df[movies_df['movie_id'] == movie_id].iloc[0]\n",
    "        \n",
    "        # Combine primary and secondary genres\n",
    "        genres = movie_info['genre_primary']\n",
    "        if pd.notna(movie_info.get('genre_secondary')):\n",
    "            genres += f\", {movie_info['genre_secondary']}\"\n",
    "        \n",
    "        recommendations.append({\n",
    "            'movie_id': movie_id,\n",
    "            'title': movie_info['title'],\n",
    "            'genres': genres,\n",
    "            'score': score\n",
    "        })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Select 3 random users for demonstration\n",
    "sample_user_indices = np.random.choice(num_users, size=3, replace=False)\n",
    "\n",
    "for i, user_idx in enumerate(sample_user_indices, 1):\n",
    "    user_id = idx_to_user[user_idx]  # Convert index to original ID\n",
    "    \n",
    "    print('\\n' + '='*70)\n",
    "    print(f'EXAMPLE {i}: User {user_id}')\n",
    "    print('='*70)\n",
    "    \n",
    "    # Show user's watch history (top 5 by strength)\n",
    "    user_history = filtered[filtered['user_id'] == user_id].nlargest(5, 'strength')\n",
    "    \n",
    "    print('\\nWatch History (Top 5):')\n",
    "    for _, row in user_history.iterrows():\n",
    "        movie_info = movies[movies['movie_id'] == row['movie_id']].iloc[0]\n",
    "        genres = movie_info['genre_primary']\n",
    "        if pd.notna(movie_info.get('genre_secondary')):\n",
    "            genres += f\", {movie_info['genre_secondary']}\"\n",
    "        print(f'  - {movie_info[\"title\"]} ({genres}) - strength: {row[\"strength\"]:.2f}')\n",
    "    \n",
    "    # Generate recommendations\n",
    "    recs = get_recommendations(user_idx, model, interaction_matrix, idx_to_item, movies, k=K)\n",
    "    \n",
    "    print(f'\\nTop-{K} Recommendations:')\n",
    "    for j, rec in enumerate(recs, 1):\n",
    "        print(f'  {j:2d}. {rec[\"title\"]} ({rec[\"genres\"]}) - score: {rec[\"score\"]:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8572884,
     "sourceId": 13502304,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
